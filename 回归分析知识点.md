此章节将记录回归分析课程中的知识点，供复习使用。

# 一元线性回归模型

## 模式表示方法

一元线性理论回归模型
$$
y = \beta_0 + \beta_1x + \varepsilon\\
E(\varepsilon|X) = 0\\
var(\varepsilon|X)=\sigma^2
$$
另一种表示方法
$$
E(y|x)=\beta_o+\beta_1x\\
var(y|x) = \sigma^2
$$
待估参数为：截距项$$\beta_0$$，斜率$$\beta_1$$，方差$$\sigma^2$$

## 误差随机项$$\varepsilon_i$$Gauss-Markov条件

高斯马尔科夫条件是最小二乘(OLS)的前提
$$
E(\varepsilon_i)=0\\
Var(\varepsilon_i) = \sigma^2\\
Cov(\varepsilon_i,\varepsilon_j)=0,i\ne j
$$

## 参数估计及其性质

### 最小二乘估计

经验回归方程通过数据的重心$$(\bar x,\bar y)$$
$$
\hat\beta_0 = \bar y -\hat\beta_1 \bar x\\
\hat\beta_1 = L_{xy} /L_{xx}\\
其中\\
L_{xx} = \sum_{i=1}^n (x_i - \bar x)^2 = \sum_{i=1}^n x_i^2 - n(\bar x)^2\\
L_{xy} = \sum_{i=1}^n (x_i - \bar x)(y_i- \bar y) =\sum_{i=1}^n x_i y_i - n \bar x \bar y
$$
$$\beta_1$$与相关系数的关系

<div align=center><img src="https://euclid-picgo.oss-cn-shenzhen.aliyuncs.com/image/202212021129600.png" alt="image-20221202112909434" style="zoom:80%;" /><\div>

### 极大似然估计

仅考虑$$\beta_0,\beta_1$$估计时，与最小二乘原理相同。进一步对$$\sigma^2$$的估计为
$$
\sigma^2 = \frac{1}{n} \sum_{i=1}^n(y_i - \hat y_i)^2 = \frac{1}{n}\sum_{i=1}^ne_i^2
$$
但是这是一个有偏估计量，所以一般采用调整后的无偏估计量作为方差的估计具体将在残差性质中证明
$$
E(\sum e_i^2) = (n-2)\sigma^2\\
\hat \sigma^2 = \frac{1}{n-2}\sum_{i=1}^n(y_i - \bar y)^2
$$

### 最小二乘估计的性质

- 线性性

  $$\beta_0,\beta_1$$是$$y_1,y_2,...,y_n$$的线性组合：
  $$
  \hat \beta_1 = \sum_{i=1}^n\frac{(x_i - \bar x)}{L_{xx}}y_i\\
  \hat \beta_0 = \sum_{i=1}^n[\frac{1}{n}-\frac{(x_i - \bar x)\bar x}{L_{xx}}]y_i
  $$
  给定$$x_i$$,拟合值$$\hat y_i$$可以表示为$$y_1,y_2,...,y_n$$的线性组合
  $$
  \hat y_i = \hat \beta_0 +\hat \beta_1 x_i = \sum_{j=1}^n [\frac{1}{n}+\frac{(x_i - \bar x)(x_j - \bar x)}{L_{xx}}]y_j \doteq \sum_{j=1}^n h_{ij}y_j
  $$
  残差$$e_i$$可以表示为$$y_1,y_2,...,y_n$$的线性组合
  $$
  e_i = y_i - \hat y_i = y_i -\sum_{j=1}^n h_{ij}y_j
  $$
  **注意**，$$h_{ij}=h_{ji}$$是只和$$x$$有关的量。

### 估计量的期望、方差、协方差

截距项$$\beta_0$$的估计量
$$
\hat \beta_0 \sim N(\beta_0,[\frac{1}{n}+\frac{\bar x ^2}{L_{xx}}]\sigma^2)
$$
斜率项$$\beta_1$$的估计量
$$
\hat \beta_1 \sim N(\beta_1,\frac{\sigma^2}{L_{xx}})
$$
估计量的协方差
$$
Cov(\hat \beta_0,\hat \beta_1) = -\frac{\bar x}{L_{xx}}\sigma^2
$$
<img src="https://euclid-picgo.oss-cn-shenzhen.aliyuncs.com/image/202212021158144.png" alt="image-20221202115833971" style="zoom:80%;" />

## 回归方程显著性检验

### 回归系数的显著性检验（t检验）

构造t统计量，其中$$\hat \sigma^2$$为$$\sigma^2$$的无偏统计量
$$
t = \frac{\hat \beta_1  -0}{\sqrt{\hat \sigma^2/L_{xx}}}
$$

### 回归方程的显著性检验（F检验）

构造F统计量服从卡方分布，右侧检验，越大越拒绝
$$
F = \frac {SSR/1}{SSE/n-2}\\其中\\
SST = \sum_{i=1}^n(y_i-\bar y)^2、SSR = \sum_{i=1}^n(\hat y_i - \bar y)^2，SSE = \sum_{i=1}^n(\hat y_i-y_i)^2
$$

## 残差及其性质

  残差$$e_i$$可以表示为$$y_1,y_2,...,y_n$$的线性组合
$$
e_i = y_i - \hat y_i = y_i -\sum_{j=1}^n h_{ij}y_j\\其中\\
h_{ij} = \frac{1}{n}+\frac{(x_i - \bar x)(x_j - \bar x)}{L_{xx}}
$$
残差$$e_i$$可以看做随机变量$$\varepsilon$$的非独立样本

### 残差的协方差

$$
当i=j时\\
Cov(e_i,e_j)=(1-h_{ii})\sigma^2 \\
当i \ne j时\\
Cov(e_i,e_j)=-h_{ij}\sigma^2
$$

### 残差平方和的期望

$$
E(\sum e_i^2) = \sum E(e_i^2) = \sum Var(e_i) = \sum (1-h_{ii})\sigma^2 = (n - \sum h_{ii})\sigma^2 = (n-2)\sigma^2
$$

### 杠杆值$$h_ij$$

度量了自变量空间中的第$$i$$个数据偏离数据中心的程度
$$
\sum_{i=1}^n h_{ii} = 2,即tr(H) = 2\\
\sum_{j=1}^n h_{ij}^2 = h_{ii}\\
\sum_{k=1}^n h_{ik}h_{kj} = h_{ij}
$$
以上性质在多元中，联系帽子矩阵$$H$$可方便理解

<img src="https://euclid-picgo.oss-cn-shenzhen.aliyuncs.com/image/202212021223257.png" alt="image-20221202122354159" style="zoom:80%;" />

## 区间预测

### 新值预测

$$
\hat y_0 = \sum_{j=1}^n h_{0j}y_j \sim N(\beta_0 + \beta_1x_0,\sum_{j =0}^n h_{0j}^2 \times\sigma^2) = N(\beta_0 + \beta_1x_0,h_{00}\times\sigma^2)\\
y_0 - \hat y_0 \sim N(0,(1+h_{00})\sigma^2)\\
t =\frac{y_0 - \hat y_0}{\sqrt{(1+h_{00})\sigma^2}}\sim t(n-2)
$$

故$$y_0$$置信水平为$$1-\alpha$$的*预测区间*为$$\hat y_0 \pm t_{\alpha/2}\times \sqrt{(1+h_{00})}\sigma$$，$$h_{00}$$称为新值$$x_0$$的杠杆值
$$
h_{00} = \frac{1}{n}+\frac{(x_0-\bar x)^2}{L_{xx}}
$$
样本量越大，$$L_{xx}$$越大，$$x_0$$越靠近$$\bar x$$，则$$h_{00}$$越小，区间长度越短，预测精度越高

### 新值平均值的预测

因为$$E(\hat y_0)$$为常量，所以方差为0
$$
y_0 - E(\hat y_0) \sim N(0,h_{00}\times\sigma^2)\\
t =\frac{y_0 - \hat y_0}{\sqrt{h_{00}\times \sigma^2}}\sim t(n-2)
$$

# 多元线性回归

## 矩阵表示

$$
Y = X\beta + \varepsilon\\
E(\varepsilon) = 0\\
Var(\varepsilon) = \sigma^2 I_n
$$



## 误差随机项$$\varepsilon$$Gauss-Markov条件

$$
E(\varepsilon) = 0\\
Var(\varepsilon) = \sigma^2I_n
$$

随机误差项满足正态分布假设，矩阵表示形式为$$\varepsilon\sim N(0,\sigma^2I_n)$$，此时$$Y\sim N(X\beta,\sigma^2I_n)$$

## 参数估计及其性质

目标函数$$Q(\hat \beta)$$最小
$$
Q(\hat \beta) = (Y - X\beta)^T(Y-X\beta)\\最小得到
\\\hat \beta =(X^TX)^{-1}X^TY
$$

### 拟合值和残差的矩阵表示

引入**帽子矩阵（投影矩阵）**$$H=X(X^TX)^{-1}X^T$$的概念，其各元素为$$h_{ij}$$，即为一元回归中的杠杆值
$$
\hat Y = X \hat \beta = X(X^TX)^{-1}X^TY \doteq HY \\
e = Y - \hat Y = (I-H)Y
$$
其中$$H$$是只和$$X$$相关的量

### 帽子矩阵$$H$$的性质

- $$H$$为对称幂等阵
- $$tr(H) =\sum_{j=1}^nhjj= p+1$$

- $$rk(H) = tr(H) = p+1$$
- $$I-H$$为对称幂等阵
- $$rk(I-H) = n-p-1$$

实对称幂等矩阵的特征根非0即1

## 方差的极大似然估计

极大似然估计量是有偏的估计
$$
\hat \sigma^2 = \frac {1}{n}(Y - X\beta)^T(Y-X\beta) = \frac{1}{n}e^Te
$$
无偏估计为
$$
\hat \sigma^2 = \frac{1}{n-p-1}e^Te = \frac{1}{n-p-1}Y^T(I_n-H)Y
$$
因为
$$
E(e^Te) = E(\sum_{i=1}^ne_i^2) = \sum_{i=1}^nE(e_i^2) =  \sum_{i=1}^nVar(e_i^2)=(n-p-1)\sigma^2
$$

## 估计量的方差矩阵

$$
Var(\hat \beta) = Cov(\hat \beta,\hat\beta) = \sigma^2(X^TX)^{-1}
$$

通常$$(X^TX)^{-1}$$为非对角矩阵，即$$\hat \beta$$各分量间具有相关性

## 残差及其性质

与一元模型一致
$$
当i=j时\\
Cov(e_i,e_j)=(1-h_{ii})\sigma^2 \\
当i \ne j时\\
Cov(e_i,e_j)=-h_{ii}\sigma^2
$$

- $$Cov(e,\hat Y) = Cov((I_n-H)Y,HY)=0$$
- $$Cov(e,\hat y_o) =Cov((I_n-H)Y,x_o(X^TX)^{-1}X^TY)=0$$
- $$Cov(e,\hat\beta) =Cov((I_n-H)Y,(X^TX)^{-1}X^TY)=0$$
- $$X^Te=0$$

## X正态假设下的分布

- 观测Y
  $$
  Y \sim N(X\beta,\sigma^2I_n)
  $$

- 参数估计值
  $$
  \hat \beta \sim N(\beta,\sigma^2 (X^TX)^{-1})
  $$

- 预测值$$\hat Y$$
  $$
  \hat Y = HY\sim N(X\beta,\sigma^2H)
  $$

- 预测单值$$\hat y_0$$
  $$
  \hat y_0 = x_0^T(X^TX)X^TY\sim N(x_0^T\beta,\sigma^2x_0^T(X^TX)^{-1}x_0)
  $$

- 残差向量$$e$$
  $$
  e = (I_n-H)Y \sim N(0,\sigma^2(I_n-H))
  $$

- $$SSE/\sigma^2$$
  $$
  \frac{(n-p-1)\hat\sigma^2}{\sigma^2}= \frac{SSE}{\sigma^2} = \frac{Y^T(I_n-H)Y}{\sigma^2} \sim \chi^2(n-p-1)
  $$

- $$\hat \sigma^2 $$与$$\hat \beta$$独立

## 回归方程的显著性检验

### F检验统计量

右侧检验，越大越拒绝，认为模型有效
$$
F = \frac{SSR/p}{SSE/(n-p-1)}
$$

- SSR
  $$
  Y^T(H-\frac{1}{n}\mathbf{1}_n\mathbf{1}_n^T)Y
  $$

  其中$$Rank(H-\frac{1}{n}\mathbf{1}_n\mathbf{1}_n^T)=p+1-1=p$$

- SSE
  $$
  Y^T(I_n-H)Y
  $$
  其中$$Rank(I_n-H)=n-p-1$$

- SST
  $$
  Y^T(I_n-\frac{1}{n}\mathbf{1}_n\mathbf{1}_n^T)Y
  $$
  

其中$$Rank(I_n-\frac{1}{n}\mathbf{1}_n\mathbf{1}_n^T)=n-1$$

## t检验

参数估计值$$\hat\beta\sim N(\beta,\sigma^2(X^TX)^{-1})$$

### 原假设下

$$
\hat\beta_j \sim N(0,\sigma^2c_{jj})
$$

其中$$c_{jj}$$表示$$(X^TX)^{-1}$$第$$j+1$$个对角线上的元素

### t统计量

$$
t_j = \frac{\hat \beta_j}{\sqrt{c_{jj}}\sigma} \sim t(n-p-1)
$$

## 拟合优度$$R^2$$

$$R^2$$表示因变量的变异中，自变量可以解释的比例
$$
R^2 = \frac{SSR}{SST}
$$

### 样本复相关系数

$$
R = \sqrt{R^2}
$$

### 自由度调整后的$$R^2$$

$$
R_a^2 = 1-\frac{n-1}{n-p-1}(1-R^2)
$$

## 区间预测

### 新值预测

构造$$t$$统计量
$$
t = \frac{y_0-\hat y_0}{\sqrt{1+x_0^T(X^TX)^{-1}x_0^T}\hat\sigma}\sim t(n-p-1)
$$
故$$y_0$$置信水平为$$1-\alpha$$的*预测区间*为$$\hat y_0 \pm t_{\alpha/2}(n-p-1)\times \sqrt{1+x_0^T(X^TX)^{-1}x_0^T}\hat\sigma$$

### 新值平均值预测

$$
t = \frac{\hat y_0-E(y_0)}{\sqrt{x_0^T(X^TX)^{-1}x_0^T}\hat\sigma}\sim t(n-p-1)
$$

故$$y_0$$置信水平为$$1-\alpha$$的*预测区间*为$$\hat y_0 \pm t_{\alpha/2}(n-p-1)\times \sqrt{x_0^T(X^TX)^{-1}x_0^T}\hat\sigma$$

## 中心化的影响

- $$X$$中心化后，斜率的估计量不变，截距项的估计值变为$$\bar y$$
- $$X$$、$$Y$$中心化，斜率的估计量不变，截距项的估计值变为0
- 事实上，任何形式的坐标平移都不影响斜率项的估计值

# 回归诊断

## 残差及残差图

### 残差值向量

$$
e = Y - \hat{Y} = Y - HY = (I-H)Y
$$

![image-20230102182419867](https://euclid-picgo.oss-cn-shenzhen.aliyuncs.com/image/202301021824931.png)

### 学生化残差

将残差进行标准化，$Var(e_i) = (1-h_{ii})\sigma^2$
$$
r_i = \frac{e_i}{\sqrt{\hat{var(e_i)}}} = \frac{e_i}{\hat{\sigma}\sqrt{1-h_{ii}}}
$$
即使$\varepsilon \sim N(0,1)$，$r_i$的分布依旧比较复杂，且诸$r_i$间彼此不独立
$$
E(r_i)=0\\
Var(e_i)=1\\
\rho(r_i.r_j)=\frac{-h_{ij}}{\sqrt{1-h_{ii}}\sqrt{1-h_{jj}}}
$$
但是在实际应用中，还是近似将$r_i$认为为服从标准正态分布

## 异常点和强影响点

## 异方差性及其处理

## 自相关性及其处理

## 多重共线性及其处理

