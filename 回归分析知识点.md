此章节将记录回归分析课程中的知识点，供复习使用。

# 一元线性回归模型

## 模式表示方法

一元线性理论回归模型
$$
y = \beta_0 + \beta_1x + \varepsilon\\
E(\varepsilon|X) = 0\\
var(\varepsilon|X)=\sigma^2
$$
另一种表示方法
$$
E(y|x)=\beta_o+\beta_1x\\
var(y|x) = \sigma^2
$$
待估参数为：截距项$$\beta_0$$，斜率$$\beta_1$$，方差$$\sigma^2$$

## 误差随机项$$\varepsilon_i$$Gauss-Markov条件

高斯马尔科夫条件是最小二乘(OLS)的前提
$$
E(\varepsilon_i)=0\\
Var(\varepsilon_i) = \sigma^2\\
Cov(\varepsilon_i,\varepsilon_j)=0,i\ne j
$$

## 参数估计及其性质

### 最小二乘估计

经验回归方程通过数据的重心$$(\bar x,\bar y)$$
$$
\hat\beta_0 = \bar y -\hat\beta_1 \bar x\\
\hat\beta_1 = L_{xy} /L_{xx}\\
其中\\
L_{xx} = \sum_{i=1}^n (x_i - \bar x)^2 = \sum_{i=1}^n x_i^2 - n(\bar x)^2\\
L_{xy} = \sum_{i=1}^n (x_i - \bar x)(y_i- \bar y) =\sum_{i=1}^n x_i y_i - n \bar x \bar y
$$
$$\beta_1$$与相关系数的关系

<img src="https://euclid-picgo.oss-cn-shenzhen.aliyuncs.com/image/202212021129600.png" alt="image-20221202112909434" style="zoom:80%;" />

### 极大似然估计

仅考虑$$\beta_0,\beta_1$$估计时，与最小二乘原理相同。进一步对$$\sigma^2$$的估计为
$$
\sigma^2 = \frac{1}{n} \sum_{i=1}^n(y_i - \hat y_i)^2 = \frac{1}{n}\sum_{i=1}^ne_i^2
$$
但是这是一个有偏估计量，所以一般采用调整后的无偏估计量作为方差的估计具体将在残差性质中证明
$$
E(\sum e_i^2) = (n-2)\sigma^2\\
\hat \sigma^2 = \frac{1}{n-2}\sum_{i=1}^n(y_i - \bar y)^2
$$

### 最小二乘估计的性质

- 线性性

  $$\beta_0,\beta_1$$是$$y_1,y_2,...,y_n$$的线性组合：
  $$
  \hat \beta_1 = \sum_{i=1}^n\frac{(x_i - \bar x)}{L_{xx}}y_i\\
  \hat \beta_0 = \sum_{i=1}^n[\frac{1}{n}-\frac{(x_i - \bar x)\bar x}{L_{xx}}]y_i
  $$
  给定$$x_i$$,拟合值$$\hat y_i$$可以表示为$$y_1,y_2,...,y_n$$的线性组合
  $$
  \hat y_i = \hat \beta_0 +\hat \beta_1 x_i = \sum_{j=1}^n [\frac{1}{n}+\frac{(x_i - \bar x)(x_j - \bar x)}{L_{xx}}]y_j \doteq \sum_{j=1}^n h_{ij}y_j
  $$
  残差$$e_i$$可以表示为$$y_1,y_2,...,y_n$$的线性组合
  $$
  e_i = y_i - \hat y_i = y_i -\sum_{j=1}^n h_{ij}y_j
  $$
  **注意**，$$h_{ij}=h_{ji}$$是只和$$x$$有关的量。

### 估计量的期望、方差、协方差

截距项$$\beta_0$$的估计量
$$
\hat \beta_0 \sim N(\beta_0,[\frac{1}{n}+\frac{\bar x ^2}{L_{xx}}]\sigma^2)
$$
斜率项$$\beta_1$$的估计量
$$
\hat \beta_1 \sim N(\beta_1,\frac{\sigma^2}{L_{xx}})
$$
估计量的协方差
$$
Cov(\hat \beta_0,\hat \beta_1) = -\frac{\bar x}{L_{xx}}\sigma^2
$$
<img src="https://euclid-picgo.oss-cn-shenzhen.aliyuncs.com/image/202212021158144.png" alt="image-20221202115833971" style="zoom:80%;" />

## 回归方程显著性检验

### 回归系数的显著性检验（t检验）

构造t统计量，其中$$\hat \sigma^2$$为$$\sigma^2$$的无偏统计量
$$
t = \frac{\hat \beta_1  -0}{\sqrt{\hat \sigma^2/L_{xx}}}
$$

### 回归方程的显著性检验（F检验）

构造F统计量服从卡方分布，右侧检验，越大越拒绝
$$
F = \frac {SSR/1}{SSE/n-2}\\其中\\
SST = \sum_{i=1}^n(y_i-\bar y)^2、SSR = \sum_{i=1}^n(\hat y_i - \bar y)^2，SSE = \sum_{i=1}^n(\hat y_i-y_i)^2
$$

## 残差及其性质

  残差$$e_i$$可以表示为$$y_1,y_2,...,y_n$$的线性组合
$$
e_i = y_i - \hat y_i = y_i -\sum_{j=1}^n h_{ij}y_j\\其中\\
h_{ij} = \frac{1}{n}+\frac{(x_i - \bar x)(x_j - \bar x)}{L_{xx}}
$$
残差$$e_i$$可以看做随机变量$$\varepsilon$$的非独立样本

### 残差的协方差

$$
当i=j时\\
Cov(e_i,e_j)=(1-h_{ii})\sigma^2 \\
当i \ne j时\\
Cov(e_i,e_j)=-h_{ii}\sigma^2
$$

### 残差平方和的期望

$$
E(\sum e_i^2) = \sum E(e_i^2) = \sum Var(e_i) = \sum (1-h_{ii})\sigma^2 = (n - \sum h_{ii})\sigma^2 = (n-2)\sigma^2
$$

### 杠杆值$$h_ij$$

度量了自变量空间中的第$$i$$个数据偏离数据中心的程度
$$
\sum_{i=1}^n h_{ii} = 2,即tr(H) = 2\\
\sum_{j=1}^n h_{ij}^2 = h_{ii}\\
\sum_{k=1}^n h_{ik}h_{kj} = h_{ij}
$$
以上性质在多元中，联系帽子矩阵$$H$$可方便理解

<img src="https://euclid-picgo.oss-cn-shenzhen.aliyuncs.com/image/202212021223257.png" alt="image-20221202122354159" style="zoom:80%;" />

